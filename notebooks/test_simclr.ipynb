{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siharini/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.0008\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Data preprocessing and loading\n",
    "def prepare_datasets(name):\n",
    "    if name == \"tiny-imagenet\":\n",
    "        \n",
    "        train_dataset = load_dataset(\"zh-plus/tiny-imagenet\", split=\"train\")\n",
    "        test_dataset = load_dataset(\"zh-plus/tiny-imagenet\", split=\"valid\")\n",
    "\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        train_dataset = CustomDataset(train_dataset, transform=train_transform)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_dataset = CustomDataset(test_dataset, transform=test_transform)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    elif name == \"stl10\":\n",
    "        train_dataset = datasets.STL10('/Users/siharini/github/DL-Project/src/data', split='train', download=False,\n",
    "                                       transform=transforms.ToTensor())\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                  num_workers=0, drop_last=False, shuffle=True)\n",
    "\n",
    "        test_dataset = datasets.STL10('/Users/siharini/github/DL-Project/src/data', split='test', download=False,\n",
    "                                      transform=transforms.ToTensor())\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, batch_size=2*batch_size,\n",
    "                                 num_workers=10, drop_last=False, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "    \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        if self.transform:\n",
    "            sample[\"image\"] = self.transform(sample[\"image\"])\n",
    "        if sample[\"image\"].shape[0] == 1:\n",
    "            sample[\"image\"] = sample[\"image\"].repeat(3, 1, 1)\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition and setup\n",
    "def initialize_model(num_classes):\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=num_classes).to(device)\n",
    "    return model\n",
    "\n",
    "def load_pretrained_weights(model, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    state_dict = checkpoint[\"state_dict\"]\n",
    "\n",
    "    for k in list(state_dict.keys()):\n",
    "        if k.startswith(\"backbone.\"):\n",
    "            if k.startswith(\"backbone\") and not k.startswith(\"backbone.fc\"):\n",
    "                # remove prefix\n",
    "                state_dict[k[len(\"backbone.\") :]] = state_dict[k]\n",
    "        del state_dict[k]\n",
    "\n",
    "    log = model.load_state_dict(state_dict, strict=False)\n",
    "    assert log.missing_keys == [\"fc.weight\", \"fc.bias\"]\n",
    "\n",
    "    # Freeze all layers but the last fully connected layer\n",
    "    for name, param in model.named_parameters():\n",
    "        if name not in [\"fc.weight\", \"fc.bias\"]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "\n",
    "def train(model, train_loader, test_loader, criterion, optimizer, epochs):\n",
    "    train_losses = []\n",
    "    train_top1_accuracies = []\n",
    "    test_top1_accuracies = []\n",
    "    test_top5_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        top1_train_accuracy = 0\n",
    "\n",
    "        for counter, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(x_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            top1 = accuracy(logits, y_batch, topk=(1,))\n",
    "            top1_train_accuracy += top1[0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "        epoch_train_loss /= (counter + 1)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        top1_train_accuracy /= (counter + 1)\n",
    "        train_top1_accuracies.append(top1_train_accuracy)\n",
    "\n",
    "        top1_accuracy, top5_accuracy = evaluate(model, test_loader)\n",
    "        test_top1_accuracies.append(top1_accuracy)\n",
    "        test_top5_accuracies.append(top5_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch}\\tTrain Loss: {epoch_train_loss:.4f}\\tTop1 Train Accuracy: {top1_train_accuracy:.2f}%\\tTop1 Test Accuracy: {top1_accuracy:.2f}%\\tTop5 Test Accuracy: {top5_accuracy:.2f}%\")\n",
    "\n",
    "    plot_loss_curves(train_losses)\n",
    "    plot_accuracy_curves(train_top1_accuracies, test_top1_accuracies, test_top5_accuracies)\n",
    "\n",
    "def plot_loss_curves(train_losses):\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy_curves(train_top1_accuracies, test_top1_accuracies, test_top5_accuracies):\n",
    "    plt.plot(train_top1_accuracies, label='Training Top-1 Accuracy')\n",
    "    plt.plot(test_top1_accuracies, label='Testing Top-1 Accuracy')\n",
    "    plt.plot(test_top5_accuracies, label='Testing Top-5 Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    top1_accuracy = 0\n",
    "    top5_accuracy = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for counter, (x_batch, y_batch) in enumerate(test_loader):\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            logits = model(x_batch)\n",
    "            top1, top5 = accuracy(logits, y_batch, topk=(1, 5))\n",
    "            top1_accuracy += top1[0]\n",
    "            top5_accuracy += top5[0]\n",
    "\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    top1_accuracy /= (counter + 1)\n",
    "    top5_accuracy /= (counter + 1)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Compute precision, recall, and F1-score\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1_score:.2f}\")\n",
    "\n",
    "    return top1_accuracy, top5_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare datasets\n",
    "dataset_name = \"stl10\"\n",
    "num_classes = 200 if dataset_name == \"tiny-imagenet\" else 10\n",
    "train_loader, test_loader = prepare_datasets(dataset_name)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = initialize_model(num_classes)\n",
    "\n",
    "# Load pretrained weights\n",
    "checkpoint_path = \"checkpoint_0100.pth.tar\"\n",
    "model = load_pretrained_weights(model, checkpoint_path)\n",
    "\n",
    "# Set up criterion and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, test_loader, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
